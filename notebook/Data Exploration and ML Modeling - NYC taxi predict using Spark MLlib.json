{
	"name": "Data Exploration and ML Modeling - NYC taxi predict using Spark MLlib",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "dnmay25spool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/c75f6bab-9d6a-4ee1-b464-a3beae73063c/resourceGroups/dnmay25Demo/providers/Microsoft.Synapse/workspaces/dnmay25demows/bigDataPools/dnmay25spool",
				"name": "dnmay25spool",
				"type": "Spark",
				"endpoint": "https://dnmay25demows.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/dnmay25spool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Predict NYC Taxi Tips using Spark ML and Azure Open Datasets\n",
					"\n",
					"The notebook ingests, visualizes, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them.\n",
					"The goal is to predict for a given trip whether there will be a trip or not.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# spark ML imports\n",
					"import matplotlib.pyplot as plt\n",
					"\n",
					"from pyspark.sql.functions import unix_timestamp\n",
					"\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.types import *\n",
					"from pyspark.sql.functions import *\n",
					"\n",
					"from pyspark.ml import Pipeline\n",
					"from pyspark.ml import PipelineModel\n",
					"from pyspark.ml.feature import RFormula\n",
					"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
					"from pyspark.ml.classification import LogisticRegression\n",
					"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
					"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# additional imports for y=this model\r\n",
					"import warnings\r\n",
					"import itertools\r\n",
					"import numpy as np\r\n",
					"import matplotlib.pyplot as plt\r\n",
					"warnings.filterwarnings(\"ignore\")\r\n",
					"plt.style.use('fivethirtyeight')\r\n",
					"import pandas as pd\r\n",
					"import statsmodels.api as sm\r\n",
					"\r\n",
					"plt.rcParams['axes.labelsize'] = 14\r\n",
					"plt.rcParams['xtick.labelsize'] = 12\r\n",
					"plt.rcParams['ytick.labelsize'] = 12\r\n",
					"plt.rcParams['text.color'] = 'k'"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Ingest DataÂ¶ \n",
					"\n",
					"Get a sample data of nyc yellow taxi to make it faster/easier to evaluate different approaches to prep for the modelling phase later in the notebook."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Write data to Azure Storage Blob\r\n",
					"\r\n",
					"Synapse leverage **Shared access signature (SAS)** to access Azure Blob Storage. To avoid exposing SAS keys in the code, we recommend creating a new linked service in Synapse workspace to the Azure Blob Storage account you want to access.\r\n",
					"\r\n",
					"Follow these steps to add a new linked service for an Azure Blob Storage account:\r\n",
					"\r\n",
					"1. Open the [Azure Synapse Studio](https://web.azuresynapse.net/).\r\n",
					"2. Select **Manage** from the left panel and select **Linked services** under the **External connections**.\r\n",
					"3. Search **Azure Blob Storage** in the **New linked Service** panel on the right.\r\n",
					"4. Select **Continue**.\r\n",
					"5. Select the Azure Blob Storage Account to access and configure the linked service name. Suggest using **Account key** for the **Authentication method**.\r\n",
					"6. Select **Test connection** to validate the settings are correct.\r\n",
					"7. Select **Create** first and click **Publish all** to save your changes.\r\n",
					"\r\n",
					"You can access data on Azure Blob Storage with Synapse Spark via following URL:\r\n",
					"\r\n",
					"```wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/```\r\n",
					"\r\n",
					"Please make sure to allow contatiner level read and write permission. Fill in the access info for your Azure storage blob in the cell below. \r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.types import *\r\n",
					"\r\n",
					"#https://dnmay25gen2.blob.core.windows.net/dnmay25users/user/agg_supply/ED33C961-7B22-439E-8FC1-642D08051DBD_70_0-1.parquet\r\n",
					"\r\n",
					"# Azure storage access info\r\n",
					"blob_account_name = 'dnmay25gen2' # replace with your blob name\r\n",
					"blob_container_name = 'dnmay25users' # replace with your container name\r\n",
					"blob_relative_path = 'user/agg_supply/' # replace with your relative folder path\r\n",
					"linked_service_name = 'AzureBlobStorage1' # replace with your linked service name\r\n",
					"\r\n",
					"blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Allow SPARK to access from Blob remotely\r\n",
					"wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
					"spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
					"print('Remote blob path: ' + wasbs_path)"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"parquet_path = wasbs_path + 'ED33C961-7B22-439E-8FC1-642D08051DBD_70_0-1.parquet'"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"df_parquet = spark.read.parquet(parquet_path)"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"#--------- 5126794 -----------------------------------------------------------------"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"print('the size of the data is: %d rows and  %d columns' % df_parquet.shape)"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": true
				},
				"source": [
					"%%sql\r\n",
					"SELECT count(*) FROM mydataframetable"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"myNewPythonDataFrame = spark.sql(\"SELECT * FROM mydataframetable\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"print(\"read data from SQL server table  \")\r\n",
					"jdbcDF = spark.read \\\r\n",
					"        .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
					"        .option(\"url\", url) \\\r\n",
					"        .option(\"dbtable\", dbtable) \\\r\n",
					"        .option(\"user\", user) \\\r\n",
					"        .option(\"password\", password).load()\r\n",
					"\r\n",
					"jdbcDF.show(5)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from datetime import datetime\n",
					"from dateutil import parser\n",
					"\n",
					"\n",
					"end_date = parser.parse('2018-05-08 00:00:00')\n",
					"start_date = parser.parse('2018-05-01 00:00:00')\n",
					"\n",
					"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
					"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"#To make development easier, faster and less expensive downsample for now\n",
					"sampled_taxi_df = nyc_tlc_df.sample(True, 0.001, seed=1234)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"#The charting package needs a Pandas dataframe or numpy array do the conversion\r\n",
					"sampled_taxi_pd_df = sampled_taxi_df.toPandas()\r\n",
					"sampled_taxi_pd_df['tpepPickupDateTime'].min(), sampled_taxi_pd_df['tpepPickupDateTime'].max()"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Ge rid of the columns we dont need and see whats left\r\n",
					"cols = ['vendorID', 'passengerCount', 'startLon', 'startLat', 'endLon', 'endLat', 'rateCodeId', 'storeAndFwdFlag', 'paymentType', 'fareAmount', 'extra', 'mtaTax', 'improvementSurcharge', 'tipAmount', 'tollsAmount', 'totalAmount']\r\n",
					"sampled_taxi_pd_df.drop(cols, axis=1, inplace=True)\r\n",
					"\r\n",
					""
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"sampled_taxi_pd_df.head(10)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Add the date part columns to the individual demand and supply dataframes"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Add some additional columns\r\n",
					"def gethour(ts):\r\n",
					"    return ts.hour\r\n",
					"\r\n",
					"def getday(ts):\r\n",
					"    return ts.day\r\n",
					"\r\n",
					"sampled_taxi_pd_df['puDay'] = sampled_taxi_pd_df['tpepPickupDateTime'].apply(getday)\r\n",
					"sampled_taxi_pd_df['puHour'] = sampled_taxi_pd_df['tpepPickupDateTime'].apply(gethour)\r\n",
					"\r\n",
					"sampled_taxi_pd_df.head(10)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# see what we have left\r\n",
					"for col in sampled_taxi_pd_df.columns:\r\n",
					"    print(col)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Lets create the inter zone trip length dataframe\r\n",
					"# So we need an average trip length per start zone - end zone pair \r\n",
					"triplength_df = sampled_taxi_pd_df.groupby(['puLocationId', 'doLocationId']).agg({'tripDistance': ['mean']})\r\n",
					"triplength_df = triplength_df.reset_index()\r\n",
					"triplength_df.head(10)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# see how many we have \r\n",
					"triplength_df.count()"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# Amazing - now create demand data frame\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Exploratory Data Analysis\n",
					"\n",
					"Look at the data and evaluate its suitability for use in a model, do this via some basic charts focussed on tip values and relationships."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Look at tips by amount count histogram\n",
					"ax1 = sampled_taxi_pd_df['tipAmount'].plot(kind='hist', bins=25, facecolor='lightblue')\n",
					"ax1.set_title('Tip amount distribution')\n",
					"ax1.set_xlabel('Tip Amount ($)')\n",
					"ax1.set_ylabel('Counts')\n",
					"plt.suptitle('')\n",
					"plt.show()\n",
					"\n",
					"# How many passengers tip'd by various amounts\n",
					"ax2 = sampled_taxi_pd_df.boxplot(column=['tipAmount'], by=['passengerCount'])\n",
					"ax2.set_title('Tip amount by Passenger count')\n",
					"ax2.set_xlabel('Passenger count') \n",
					"ax2.set_ylabel('Tip Amount ($)')\n",
					"plt.suptitle('')\n",
					"plt.show()\n",
					"\n",
					"# Look at the relationship between fare and tip amounts\n",
					"ax = sampled_taxi_pd_df.plot(kind='scatter', x= 'fareAmount', y = 'tipAmount', c='blue', alpha = 0.10, s=2.5*(sampled_taxi_pd_df['passengerCount']))\n",
					"ax.set_title('Tip amount by Fare amount')\n",
					"ax.set_xlabel('Fare Amount ($)')\n",
					"ax.set_ylabel('Tip Amount ($)')\n",
					"plt.axis([-2, 80, -2, 20])\n",
					"plt.suptitle('')\n",
					"plt.show()"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Data Prep and Featurization\n",
					"\n",
					"It's clear from the visualizations above that there are a bunch of outliers in the data. These will need to be filtered out in addition there are extra variables that are not going to be useful in the model we build at the end.\n",
					"\n",
					"Finally there is a need to create some new (derived) variables that will work better with the model.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\\\n",
					"                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\\\n",
					"                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\\\n",
					"                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\\\n",
					"                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\\\n",
					"                                , (when(col('tipAmount') > 0, 1).otherwise(0)).alias('tipped')\n",
					"                                )\\\n",
					"                        .filter((sampled_taxi_df.passengerCount > 0) & (sampled_taxi_df.passengerCount < 8)\\\n",
					"                                & (sampled_taxi_df.tipAmount >= 0) & (sampled_taxi_df.tipAmount <= 25)\\\n",
					"                                & (sampled_taxi_df.fareAmount >= 1) & (sampled_taxi_df.fareAmount <= 250)\\\n",
					"                                & (sampled_taxi_df.tipAmount < sampled_taxi_df.fareAmount)\\\n",
					"                                & (sampled_taxi_df.tripDistance > 0) & (sampled_taxi_df.tripDistance <= 100)\\\n",
					"                                & (sampled_taxi_df.rateCodeId <= 5)\n",
					"                                & (sampled_taxi_df.paymentType.isin({\"1\", \"2\"}))\n",
					"                                )"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Data Prep and Featurization Part 2\n",
					"\n",
					"Having created new variables its now possible to drop the columns they were derived from so that the dataframe that goes into the model is the smallest in terms of number of variables, that is required.\n",
					"\n",
					"Also create some more features based on new columns from the first round.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\\\n",
					"                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\\\n",
					"                                                , when((taxi_df.pickupHour <= 6) | (taxi_df.pickupHour >= 20),\"Night\")\\\n",
					"                                                .when((taxi_df.pickupHour >= 7) & (taxi_df.pickupHour <= 10), \"AMRush\")\\\n",
					"                                                .when((taxi_df.pickupHour >= 11) & (taxi_df.pickupHour <= 15), \"Afternoon\")\\\n",
					"                                                .when((taxi_df.pickupHour >= 16) & (taxi_df.pickupHour <= 19), \"PMRush\")\\\n",
					"                                                .otherwise(0).alias('trafficTimeBins')\n",
					"                                              )\\\n",
					"                                       .filter((taxi_df.tripTimeSecs >= 30) & (taxi_df.tripTimeSecs <= 7200))"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Encoding\n",
					"\n",
					"Different ML algorithms support different types of input, for this example Logistic Regression is being used for Binary Classification. This means that any Categorical (string) variables must be converted to numbers.\n",
					"\n",
					"The process is not as simple as a \"map\" style function as the relationship between the numbers can introduce a bias in the resulting model, the approach is to index the variable and then encode using a std approach called One Hot Encoding.\n",
					"\n",
					"This approach requires the encoder to \"learn\"/fit a model over the data in the Spark instance and then transform based on what was learnt.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# The sample uses an algorithm that only works with numeric features convert them so they can be consumed\n",
					"sI1 = StringIndexer(inputCol=\"trafficTimeBins\", outputCol=\"trafficTimeBinsIndex\"); \n",
					"en1 = OneHotEncoder(dropLast=False, inputCol=\"trafficTimeBinsIndex\", outputCol=\"trafficTimeBinsVec\");\n",
					"sI2 = StringIndexer(inputCol=\"weekdayString\", outputCol=\"weekdayIndex\"); \n",
					"en2 = OneHotEncoder(dropLast=False, inputCol=\"weekdayIndex\", outputCol=\"weekdayVec\");\n",
					"\n",
					"# Create a new dataframe that has had the encodings applied\n",
					"encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Generation of Testing and Training Data Sets\n",
					"Simple split, 70% for training and 30% for testing the model. Playing with this ratio may result in different models.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Decide on the split between training and testing data from the dataframe \n",
					"trainingFraction = 0.7\n",
					"testingFraction = (1-trainingFraction)\n",
					"seed = 1234\n",
					"\n",
					"# Split the dataframe into test and training dataframes\n",
					"train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Train the Model\n",
					"\n",
					"Train the Logistic Regression model and then evaluate it using Area under ROC as the metric."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"## Create a new LR object for the model\n",
					"logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')\n",
					"\n",
					"## The formula for the model\n",
					"classFormula = RFormula(formula=\"tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec\")\n",
					"\n",
					"## Undertake training and create an LR model\n",
					"lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\n",
					"\n",
					"## Saving the model is optional but its another for of inter session cache\n",
					"datestamp = datetime.now().strftime('%m-%d-%Y-%s');\n",
					"fileName = \"lrModel_\" + datestamp;\n",
					"logRegDirfilename = fileName;\n",
					"lrModel.save(logRegDirfilename)\n",
					"\n",
					"## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC\n",
					"predictions = lrModel.transform(test_data_df)\n",
					"predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\n",
					"metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
					"print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Evaluate and Visualize\n",
					"\n",
					"Plot the actual curve to develop a better understanding of the model.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"## Plot the ROC curve, no need for pandas as this uses the modelSummary object\n",
					"modelSummary = lrModel.stages[-1].summary\n",
					"\n",
					"plt.plot([0, 1], [0, 1], 'r--')\n",
					"plt.plot(modelSummary.roc.select('FPR').collect(),\n",
					"         modelSummary.roc.select('TPR').collect())\n",
					"plt.xlabel('False Positive Rate')\n",
					"plt.ylabel('True Positive Rate')\n",
					"plt.show()"
				],
				"execution_count": 10
			}
		]
	}
}